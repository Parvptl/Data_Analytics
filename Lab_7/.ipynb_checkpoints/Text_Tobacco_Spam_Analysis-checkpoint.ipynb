{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4e9058",
   "metadata": {},
   "source": [
    "# Text, Tobacco & Spam Analysis — Lab 7\n",
    "\n",
    "**Student:** Parv  \n",
    "**Generated on:** 2025-11-12 06:16:04\n",
    "\n",
    "**Note:** This notebook uses `dataset_reviews.csv` (as a local IMDB-like sample), `GYTS.csv`, and `spam.csv` which should be present in `/mnt/data/`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16010050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "plt.rcParams.update({'figure.dpi':120, 'font.size':10})\n",
    "\n",
    "print('Imports ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b58a20",
   "metadata": {},
   "source": [
    "## Part I — Text Preprocessing & Feature Extraction (6 marks)\n",
    "Using `dataset_reviews.csv` (local sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample reviews\n",
    "reviews_path = '/mnt/data/dataset_reviews.csv'\n",
    "reviews = pd.read_csv(reviews_path)\n",
    "print('Columns:', reviews.columns.tolist())\n",
    "print('Total reviews available:', len(reviews))\n",
    "\n",
    "# Take sample of 1000 (or all if <1000)\n",
    "sample = reviews.sample(n=min(1000, len(reviews)), random_state=42).reset_index(drop=True)\n",
    "print('Sample size:', sample.shape)\n",
    "\n",
    "# Show first two reviews for POS tagging later\n",
    "sample.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fb338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "import string\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_en = set(stopwords.words('english')) if 'stopwords' in globals() else set()\n",
    "\n",
    "_contractions = {\n",
    "    \"don't\":\"do not\",\"can't\":\"cannot\",\"i'm\":\"i am\",\"it's\":\"it is\",\"you're\":\"you are\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = text.lower()\n",
    "    for k,v in _contractions.items():\n",
    "        text = text.replace(k, v)\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = expand_contractions(str(text))\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_en]\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in stems]\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Apply cleaning on sample (this may take time)\n",
    "sample['cleaned'] = sample.iloc[:,0].astype(str).apply(clean_text)\n",
    "print('Cleaned sample (first 5):')\n",
    "print(sample['cleaned'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c3029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: BoW and n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), max_features=5000)\n",
    "dtm = vectorizer.fit_transform(sample['cleaned'])\n",
    "bow_vocab = vectorizer.get_feature_names_out()\n",
    "word_counts = np.asarray(dtm.sum(axis=0)).ravel()\n",
    "freq_df = pd.DataFrame({'word': bow_vocab, 'count': word_counts}).sort_values('count', ascending=False)\n",
    "print('Top 5 words:')\n",
    "print(freq_df.head(5))\n",
    "\n",
    "# Bigrams and trigrams\n",
    "vect_2 = CountVectorizer(ngram_range=(2,2), max_features=2000)\n",
    "bi = vect_2.fit_transform(sample['cleaned'])\n",
    "bi_df = pd.DataFrame({'bigram': vect_2.get_feature_names_out(), 'count': np.asarray(bi.sum(axis=0)).ravel()}).sort_values('count', ascending=False)\n",
    "print('\\nTop 5 bigrams:')\n",
    "print(bi_df.head(5))\n",
    "\n",
    "vect_3 = CountVectorizer(ngram_range=(3,3), max_features=1000)\n",
    "tri = vect_3.fit_transform(sample['cleaned'])\n",
    "tri_df = pd.DataFrame({'trigram': vect_3.get_feature_names_out(), 'count': np.asarray(tri.sum(axis=0)).ravel()}).sort_values('count', ascending=False)\n",
    "print('\\nTop 3 trigrams:')\n",
    "print(tri_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec534c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging for first two original reviews\n",
    "from nltk import pos_tag, word_tokenize\n",
    "orig_texts = sample.iloc[:2,0].astype(str).tolist()\n",
    "for i,t in enumerate(orig_texts):\n",
    "    toks = word_tokenize(t)\n",
    "    print(f'POS tags for review {i+1}:')\n",
    "    print(pos_tag(toks)[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a9e74",
   "metadata": {},
   "source": [
    "## Part II — GYTS (Youth Tobacco Awareness) (6 marks)\n",
    "Using `GYTS.csv`. Analysis at the India level (Area == 'Total', State/UT == 'India')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f7332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GYTS\n",
    "gyts = pd.read_csv('/mnt/data/GYTS.csv')\n",
    "print('GYTS columns:', gyts.columns.tolist())\n",
    "\n",
    "# Filter for India and Area == Total\n",
    "gyts_ind = gyts[(gyts['State/UT'].str.strip().str.lower()=='india') & (gyts['Area'].str.strip().str.lower()=='total')]\n",
    "print('Rows for India Area=Total:', len(gyts_ind))\n",
    "\n",
    "gyts_ind.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waffle chart requires counts for exposure sources\n",
    "# Identify Exposure columns (columns containing 'Exposure' or similar)\n",
    "exp_cols = [c for c in gyts.columns if 'exposure' in c.lower() or 'exposed' in c.lower()]\n",
    "print('Exposure columns found:', exp_cols)\n",
    "if exp_cols and len(gyts_ind)>0:\n",
    "    vals = gyts_ind[exp_cols].iloc[0]\n",
    "    print('Exposure values:')\n",
    "    print(vals)\n",
    "else:\n",
    "    print('No exposure columns detected or no India row found; please inspect the GYTS file.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e295e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorization of usage columns (India excluded) - filter Area == Total and State/UT != India\n",
    "from scipy.stats.mstats import winsorize\n",
    "usage_cols = [c for c in gyts.columns if any(x in c.lower() for x in ['current tobacco', 'current smoker', 'cigarette', 'bidi', 'smokeless'])]\n",
    "print('Detected usage columns:', usage_cols)\n",
    "\n",
    "subset = gyts[(gyts['State/UT'].str.strip().str.lower()!='india') & (gyts['Area'].str.strip().str.lower()=='total')]\n",
    "print('Rows for states (Area=Total):', len(subset))\n",
    "orig_means = subset[usage_cols].mean()\n",
    "# apply winsorization at 5% both tails\n",
    "wins = subset[usage_cols].apply(lambda col: winsorize(col.dropna(), limits=[0.05,0.05]) if col.notna().any() else col)\n",
    "wins_means = pd.DataFrame({ 'original_mean': orig_means, 'winsorized_mean': [w.mean() if hasattr(w,'mean') else np.nan for w in wins.values.T] }, index=usage_cols)\n",
    "print(wins_means)\n",
    "\n",
    "# Plot comparison\n",
    "wins_means.plot(kind='bar', figsize=(8,4))\n",
    "plt.title('Original vs Winsorized means (states, Area=Total)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9fb8b",
   "metadata": {},
   "source": [
    "## Part III — Spam vs Ham (6 marks)\n",
    "Using `spam.csv`. Clean text, save cleaned csv, and generate two word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ec67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spam dataset\n",
    "spam = pd.read_csv('/mnt/data/spam.csv', encoding='latin-1')\n",
    "# Try to identify label and message columns\n",
    "label_col = None\n",
    "msg_col = None\n",
    "for c in spam.columns:\n",
    "    if 'label' in c.lower() or 'v1'==c.lower():\n",
    "        label_col = c\n",
    "    if 'message' in c.lower() or 'v2'==c.lower():\n",
    "        msg_col = c\n",
    "print('Detected:', label_col, msg_col)\n",
    "spam = spam[[label_col, msg_col]].rename(columns={label_col:'label', msg_col:'message'})\n",
    "spam = spam.dropna()\n",
    "spam['label'] = spam['label'].str.strip().str.lower()\n",
    "\n",
    "# Cleaning function\n",
    "def clean_msg(s):\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'[^a-z\\s]', ' ', s)\n",
    "    toks = [w for w in s.split() if w not in ENGLISH_STOP_WORDS]\n",
    "    return ' '.join(toks)\n",
    "\n",
    "spam['cleaned'] = spam['message'].apply(clean_msg)\n",
    "spam.to_csv('/mnt/data/spam_cleaned.csv', index=False)\n",
    "print('Saved cleaned spam to /mnt/data/spam_cleaned.csv')\n",
    "spam.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word clouds for top 100 longest spam and ham messages\n",
    "from wordcloud import WordCloud\n",
    "spam_msgs = spam[spam['label']=='spam']['cleaned'].sort_values(key=lambda s: s.str.len(), ascending=False).head(100).str.cat(sep=' ')\n",
    "ham_msgs = spam[spam['label']=='ham']['cleaned'].sort_values(key=lambda s: s.str.len(), ascending=False).head(100).str.cat(sep=' ')\n",
    "\n",
    "wc_spam = WordCloud(width=600, height=400).generate(spam_msgs)\n",
    "wc_ham = WordCloud(width=600, height=400).generate(ham_msgs)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(wc_spam, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Spam (top 100 longest)')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(wc_ham, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Ham (top 100 longest)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1ba86e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Lab 7 notebook.**\n",
    "\n",
    "If you want, I can run this notebook here and paste the exact outputs (tables and plots). Would you like me to execute it now?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
